{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How “good” is your model, and how can you make it better? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What distinguishes “true artists” from “one-hit wonders” in machine learning is an understanding of how a model performs with respect to different data. This hands-on tutorial will show you how to use scikit-learn’s model evaluation functions to evaluate different models in terms of accuracy and generalisability, and search for optimal parameter configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.cross_validation as cv\n",
    "\n",
    "# Extra plotting functionality\n",
    "import visplots\n",
    "\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats.distributions import randint\n",
    "from multilayer_perceptron import multilayer_perceptron\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring and pre-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will be using throughout this workshop is an adapted version of the wine quality case study, available from the UCI Machine Learning repository at https://archive.ics.uci.edu/ml/datasets/Wine+Quality. The first thing you will need to do in order to work with the wine dataset is to read the contents from the provided wine.csv data file using the `read_csv` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wine   = pd.read_csv(\"data/wine.csv\")\n",
    "header = wine.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should try to explore the first few rows of the imported wine DataFrame using the \"`head`\" function from the `pandas` package (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Write your code here ###\n",
    "\n",
    "# Solution #\n",
    "wine.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed the data into our classification models, the imported wine DataFrame needs to be converted into a `numpy` array. Subsequently, we need to split our initial dataset into the data matrix X (independent variable) and the associated class vector y (dependent or target variable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "npArray = np.array(wine)\n",
    "\n",
    "X = npArray[:,:-1]\n",
    "y = npArray[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good practice to check the dimensionality of the imported data prior to constructing any Machine Learning models. <br/> Try printing the size of the input matrix X and class vector y using the \"`shape`\" command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"X dimensions:\", ### Write your code here ###\n",
    "print \"y dimensions:\", ### Write your code here ###\n",
    "\n",
    "# Solution #\n",
    "# print \"X dimensions:\", X.shape \n",
    "# print \"y dimensions:\", y.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the class vector y, the wine samples are classified into two distinct categories of high quality (class 1) and low quality (class 0).\n",
    "<br/><br/>An important thing to understand before applying any classification algorithms is how the output labels are distributed. Are they evenly distributed? Imbalances in distribution of labels can often lead to poor classification results for the minority class even if the classification results for the majority class are very good. For the purposes of this workshop, the ratio in-between the two classes has been kept constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yFreq = scipy.stats.itemfreq(y)\n",
    "print yFreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually advisable to scale your data prior to fitting a classification model. The main advantage of scaling is to avoid attributes of greater numeric ranges dominating those in smaller numeric ranges. For the purposes of this case study, we are applying auto-scaling on the whole X dataset. (Auto-scaling: mean-centering is initially applied per column, followed by scaling where the centered columns are divided by their standard deviation). \n",
    "\n",
    "Use as a reference the `sklearn` preprocessing documentation page in order to scale your data (http://scikit-learn.org/stable/modules/preprocessing.html)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Write your code here ###\n",
    "\n",
    "# Solution #\n",
    "X = preprocessing.StandardScaler().fit_transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualise the relationship between two variables (features) using a simple scatter plot. This step can give you a good first indication of the model and the complexity (linear vs. non-linear) of the algorithm you may need to investigate. At this stage, let’s plot the first two variables against each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f0 = 0 \n",
    "f1 = 1\n",
    "\n",
    "plt.scatter(X[y==0, f0], X[y==0, f1], color = 'b', edgecolors='black', label='Low Quality')\n",
    "plt.scatter(X[y==1, f0], X[y==1, f1], color = 'r', edgecolors='black', label='High Quality')\n",
    "plt.xlabel(header[f0])\n",
    "plt.ylabel(header[f1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the values of *f0* and *f1* to values of your own choice in order to investigate the relationship between different features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training and testing a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing a classification model on the same dataset is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data (poor generalisation). To use different datasets for training and testing, we need to split the wine dataset into two disjoint sets: train and test (**Holdout method**). <br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XTrain, XTest, yTrain, yTest = cv.train_test_split(X, y, test_size= 0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XTrain and yTrain are the two arrays you use to train your model. XTest and yTest are the two arrays that you use to evaluate your model. By default, scikit-learn splits the data so that 25% of it is used for testing, but you can also specify the proportion of data you want to use for training and testing (in this case, 30% is used for testing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the sizes of the different training and test sets by using the shape attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"XTrain dimensions:\", XTrain.shape\n",
    "print \"yTrain dimensions:\", yTrain.shape\n",
    "print \"XTest dimensions:\",  XTest.shape\n",
    "print \"yTest dimensions:\",  yTest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build KNN models using scikit-learn, you will be using the `KNeighborsClassifier` function, which allows you to set the value of K using the `n_neighbors` parameter. The optimal choice of the value K is highly data-dependent: in general a larger K suppresses the effects of noise, but makes the classification boundaries less distinct. <br/>\n",
    "\n",
    "### 4.1 Uniform weights\n",
    "\n",
    "We are going to start by trying two predefined random values of K and compare their performance. For every classification model built with sklearn, we will mainly come across four main steps: 1) Building the classification model using default, pre-defined or optimised parameters, 2) Training, 3) Testing, and <br/> 4) Reporting and evaluating the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the classifier \n",
    "knn3 = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train (fit) the model\n",
    "knn3.fit(XTrain, yTrain)\n",
    "\n",
    "# Test (predict)\n",
    "yPredK3 = knn3.predict(XTest)\n",
    "\n",
    "# Report the performance metrics\n",
    "print metrics.classification_report(yTest, yPredK3)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, yPredK3), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the classification boundary created by the KNN classifier using the built-in function `visplots.knnDecisionPlot`. For easier visualisation, only the test samples are depicted in the plot. Remember though that the decision boundary has been built using the _training_ data! <br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visplots.knnDecisionPlot(XTrain, yTrain, XTest, yTest, n_neighbors= 3, weights=\"uniform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try a larger number of K, for instance K = 99 (or an *odd* number of your own choice). Can you generate the KNN model and print the metrics for a larger K using as guidance the previous example? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################################  \n",
    "# Write your code here \n",
    "# 1. Build the KNN classifier for larger K\n",
    "# 2. Train (fit) the model\n",
    "# 3. Test (predict)\n",
    "# 4. Report the performance metrics\n",
    "############################################\n",
    "\n",
    "\n",
    "###  Solution ### \n",
    "knn99 = KNeighborsClassifier(n_neighbors=99)\n",
    "knn99.fit(XTrain, yTrain)\n",
    "yPredK99 = knn99.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, yPredK99)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, yPredK99), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to visualise the boundaries as before using the K neighbours of your choice and the `knnDecisionPlot` command from  `visplots`. What do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Write your code here ### \n",
    "\n",
    "###  Solution ### \n",
    "visplots.knnDecisionPlot(XTrain, yTrain, XTest, yTest, n_neighbors= 99, weights=\"uniform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer: <BR/> For smaller values of K the decision boundaries present many \"creases\". In this case the models may suffer from instances of overfitting. For larger values of K, we can see that the decision boundaries are less distinct and tend towards linearity. In these cases the boundaries may be too simple and unable to learn thus leading to cases of underfitting. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Distance weights\n",
    "\n",
    "Under some circumstances, it is better to give more importance (\"weight\" in computing terms) to nearer neighbours. When weights = \"distance\", weights are assigned to the training data points in a way that is proportional to the inverse of the distance from the query point. In other words, nearer neighbours contribute more to the fit. <br/>\n",
    "\n",
    "What if we use weights based on distance? Does it improve the overall performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the classifier with two parameters\n",
    "knnW3 = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
    "\n",
    "# Train (fit) the model\n",
    "knnW3.fit(XTrain, yTrain)\n",
    "\n",
    "# Test (predict)\n",
    "predictedW3 = knnW3.predict(XTest)\n",
    "\n",
    "# Report the performance metrics\n",
    "print metrics.classification_report(yTest, predictedW3)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, predictedW3), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Tuning KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn library provides the grid search function `GridSearchCV` (http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html), which allows us to search for the optimum\n",
    "combination of parameters by evaluating models trained with a particular algorithm with all provided parameter combinations. Further details and examples on grid search with sklearn can be found at http://scikit-learn.org/stable/modules/grid_search.html <Br/>\n",
    "\n",
    "You can use the `GridSearchCV` function to search for a parametisation of the KNN algorithm that gives a more optimal model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the parameters to be optimised and their values/ranges\n",
    "n_neighbors = np.arange(1, 51, 2)  \n",
    "weights     = ['uniform','distance']\n",
    "\n",
    "# Construct a dictionary of hyperparameters\n",
    "parameters = [{'n_neighbors': n_neighbors, 'weights': weights}]\n",
    "\n",
    "# Apply a grid search with 10-fold cross-validation using the dictionary of parameters\n",
    "grid = GridSearchCV(KNeighborsClassifier(), parameters, cv=10)\n",
    "grid.fit(XTrain, yTrain)\n",
    "\n",
    "# Print the optimal parameters\n",
    "print \"Best parameters: n_neighbors=\", grid.best_params_['n_neighbors'], \"and weight=\", grid.best_params_['weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> Let us graphically represent the results using a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grid_scores_ contains parameter settings and scores\n",
    "scores = [x[1] for x in grid.grid_scores_]\n",
    "scores = np.array(scores).reshape(len(n_neighbors), len(weights))\n",
    "scores = np.transpose(scores)\n",
    "\n",
    "# Make a heatmap with the performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(scores, interpolation='nearest', origin='higher', cmap=plt.cm.get_cmap('jet_r'))\n",
    "plt.xticks(np.arange(len(n_neighbors)), n_neighbors)\n",
    "plt.yticks(np.arange(len(weights)), weights)\n",
    "plt.xlabel('Number of K nearest neighbors')\n",
    "plt.ylabel('Weights')\n",
    "\n",
    "# Add the colorbar\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Classification Accuracy', rotation=270, labelpad=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process (XTest). <Br/>\n",
    "So, we are testing our independent XTest dataset using the optimised model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the classifier using the optimal parameters detected by grid search \n",
    "knn = KNeighborsClassifier(n_neighbors=grid.best_params_['n_neighbors'], weights = grid.best_params_['weights'])\n",
    "\n",
    "# Train (fit) the model\n",
    "knn.fit(XTrain, yTrain)\n",
    "\n",
    "# Test (predict)\n",
    "yPredKnn = knn.predict(XTest)\n",
    "\n",
    "# Report the performance metrics\n",
    "print metrics.classification_report(yTest, yPredKnn)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, yPredKnn), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized search on hyperparameters. \n",
    "Unlike `GridSearchCV`, `RandomizedSearchCV` does not exhaustively try all the parameter settings. Instead, it samples a fixed number of parameter settings from the specified distributions. The number of parameter settings that are tried is given by `n_iter`. If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. You should use continuous distributions for continuous parameters. Further details can be found at http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_dist = {'n_neighbors': randint(1,200)}\n",
    "random_search = RandomizedSearchCV(KNeighborsClassifier(), param_distributions=param_dist, n_iter=20)\n",
    "random_search.fit(XTrain, yTrain)\n",
    "\n",
    "print \"Best parameters: n_neighbors=\", random_search.best_params_['n_neighbors']\n",
    "\n",
    "neig = [score_tuple[0]['n_neighbors'] for score_tuple in random_search.grid_scores_]\n",
    "res = [score_tuple[1] for score_tuple in random_search.grid_scores_]\n",
    "plt.scatter(neig, res)\n",
    "plt.xlabel('Number of K nearest neighbors')\n",
    "plt.ylabel('Classification Accuracy')\n",
    "plt.xlim(0,200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Exercise (or different title?)\n",
    "\n",
    "At this point you need to choose ... blah blah blah .... \n",
    "\n",
    "### 5.1 Random Forests\n",
    "\n",
    "The random forests model aggregates a group of decision trees into an ensemble. ......\n",
    "\n",
    "One of the most important tuning parameters in building a random forest is the number of trees to construct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################################   \n",
    "# Write your code here \n",
    "# 1. Build the RF classifier using the default parameters\n",
    "# 2. Train (fit) the model\n",
    "# 3. Test (predict)\n",
    "# 4. Report the performance metrics\n",
    "#############################################################\n",
    "\n",
    "## Solution ## \n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(XTrain, yTrain)\n",
    "predRF = clf.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, predRF)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, predRF),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the parameters to be optimised and their values/ranges\n",
    "\n",
    "parameters = [{\"n_estimators\": [250, 500, 1000]}]\n",
    "sample_leaf_options = [1,5,10,50,100,200,500]\n",
    "\n",
    "###### WHAT ELSE DO WE ADD HERE?!?!?!!? ###### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Support Vector Machines (SVMs)\n",
    "\n",
    "SVMs attempt to build a decision boundary that accurately separates the samples of different classes by *maximizing* the margin between them.\n",
    "\n",
    "#### Linear SVMs\n",
    "\n",
    "The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C tolerates training misclassifications and allows softer margins, while for high C the misclassifications become more significant leading to hard-margin SVMs and potentially cases of overfitting. \n",
    "\n",
    "In this example, we will use linear SVMs with the default value for C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################### \n",
    "# Write your code here \n",
    "# 1. Build a linear SVM classifier using the default parameters\n",
    "# 2. Train (fit) the model\n",
    "# 3. Test (predict)\n",
    "# 4. Report the performance metrics\n",
    "##################################################################\n",
    "\n",
    "## Solution ## \n",
    "linearSVM = SVC(kernel='linear')\n",
    "linearSVM.fit(XTrain, yTrain)\n",
    "yPredLinear = linearSVM.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, yPredLinear)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, yPredLinear),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the classification boundary created by the linear SVM using the following function. For easier visualisation, only the test samples have been included in the plot. And remember that the decision boundary has been built using the _training_ data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visplots.svmDecisionPlot(XTrain, yTrain, XTest, yTest, 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linear SVMs\n",
    "\n",
    "In addition to C, which is common for all types of SVM, the gamma parameter in the RBF kernel controls the nonlinearity of the SVM bounaries. The larger the gamma, the more nonlinear the boundaries surrounding individual samples. Lower values of gamma lead to broader, more linear boundaries. <br/><br/>  In this example, we will use non-linear SVMs with the default values for C and gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################################  \n",
    "# Write your code here \n",
    "# 1. Build the RBF SVM classifier using the default parameters\n",
    "# 2. Train (fit) the model\n",
    "# 3. Test (predict)\n",
    "# 4. Report the performance metrics\n",
    "################################################################# \n",
    "\n",
    "## Solution ## \n",
    "rbfSVM = SVC(kernel='rbf', C=1.0, gamma=0.0)\n",
    "rbfSVM.fit(XTrain, yTrain)\n",
    "yPredRBF = rbfSVM.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, yPredRBF)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, yPredRBF),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the classification boundary created by the RBF SVM using the following function. Once more, for easier visualisation, only the test samples have been included in the plot. And remember that the decision boundary has been built using the _training_ data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visplots.svmDecisionPlot(XTrain, yTrain, XTest, yTest, 'rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning for non-linear SVMs\n",
    "\n",
    "Proper choice of C and gamma is critical for the performance of SVMs. Optimisation (tuning) of the hyperparameters can be achieved by applying a coarse tuning (often followed by a finer-tuning in the \"neighborhood\" of good parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the parameters to be optimised and their values/ranges\n",
    "# Range for gamma and Cost hyperparameters\n",
    "g_range = 2. ** np.arange(-15, 5, step=2)\n",
    "C_range = 2. ** np.arange(-5, 15, step=2)\n",
    "\n",
    "# Construct a dictionary of hyperparameters as in task 4.3\n",
    "parameters = [{'gamma': g_range, 'C': C_range}] # Solution \n",
    "\n",
    "# Apply a grid search with 10-fold cross-validation using the dictionary of parameters\n",
    "grid = GridSearchCV(SVC(), parameters, cv= 10) # Solution \n",
    "grid.fit(XTrain, yTrain) # Solution \n",
    "\n",
    "# Print the optimal parameters\n",
    "bestG = np.log2(grid.best_params_['gamma']);\n",
    "bestC = np.log2(grid.best_params_['C']);\n",
    "\n",
    "print \"The best parameters are: gamma=\", bestG, \" and Cost=\", bestC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results of the grid search using a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Write your code here ### \n",
    "\n",
    "\n",
    "###  Solution ### \n",
    "# grid_scores_ contains parameter settings and scores\n",
    "scores = [x[1] for x in grid.grid_scores_]\n",
    "scores = np.array(scores).reshape(len(C_range), len(g_range))\n",
    "\n",
    "# Make a heatmap with the performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(scores, interpolation='nearest', origin='higher', cmap=plt.cm.get_cmap('jet_r'))\n",
    "plt.xticks(np.arange(len(g_range)), np.log2(g_range))\n",
    "plt.yticks(np.arange(len(C_range)), np.log2(C_range))\n",
    "plt.xlabel('gamma (log2)')\n",
    "plt.ylabel('Cost (log2)')\n",
    "\n",
    "# Add the colorbar\n",
    "\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Classification Accuracy', rotation=270, labelpad=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, testing with the optimised model (best hyperparameters) for C and gamma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################################################################  \n",
    "# Write your code here \n",
    "# 1. Build the classifier using the optimal parameters detected by grid search \n",
    "# 2. Train (fit) the model\n",
    "# 3. Test (predict)\n",
    "# 4. Report the performance metrics\n",
    "####################################################################################  \n",
    "\n",
    "\n",
    "## Solution ## \n",
    "rbfSVM = SVC(kernel='rbf', C=grid.best_params_['C'], gamma=grid.best_params_['gamma'])\n",
    "rbfSVM.fit(XTrain, yTrain)\n",
    "predictions = rbfSVM.predict(XTest) \n",
    "\n",
    "print metrics.classification_report(yTest, predictions)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, predictions),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Logistic Regression\n",
    "\n",
    "Logistic regression predicts the probability that a sample belongs to a class based on the values of the input variables, based on a linear model. In the case of classification, we can use this to then assign the sample to the most likely class.\n",
    "\n",
    "Building a logistic regression model with default parameters .. blah blah.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# Write your code here \n",
    "# 1. Build the Logistic Regression classifier using the default parameters\n",
    "# 2. Train (fit) the model\n",
    "# 3. Test (predict)\n",
    "# 4. Report the performance metrics\n",
    "#############################################################################\n",
    "\n",
    "## Solution ## \n",
    "l_regression = LogisticRegression()\n",
    "l_regression.fit(XTrain, yTrain)\n",
    "l_prediction = l_regression.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, l_prediction)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, l_prediction),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the classification boundary created by the logistic regression model using the built-in function `visplots.logregDecisionPlot`. <br/> As with the above examples, only the test samples have been included in the plot. Remember that the decision boundary has been built using the _training_ data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Write your code here ### \n",
    "\n",
    "## Solution ## \n",
    "visplots.logregDecisionPlot(XTrain, yTrain, XTest, yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two hyperparameters that are often tuned for logistic regression models are the norm used in penalisation (`penalty`), which can be either `l1` or `l2` (default `l2`) and the inverse of regularisation strength, `C` (default `1.0`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the parameters to be optimised and their values/ranges\n",
    "# Range for pen and C hyperparameters\n",
    "pen = ['l1','l2']\n",
    "C_range = 2. ** np.arange(-5, 15, step=2)\n",
    "\n",
    "# Construct a dictionary of hyperparameters as in task 4.3\n",
    "parameters = [{'C': C_range, 'penalty': pen}]\n",
    "\n",
    "\n",
    "# Apply a grid search with 10-fold cross-validation using the dictionary of parameters\n",
    "grid = GridSearchCV(LogisticRegression(), parameters, cv= 10)\n",
    "grid.fit(XTrain, yTrain)\n",
    "\n",
    "\n",
    "# Print the optimal parameters\n",
    "print \"The best parameters are: cost=\", grid.best_params_['C'], \" and penalty=\", grid.best_params_['penalty']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results of the grid search with a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grid_scores_ contains parameter settings and scores\n",
    "scores = [x[1] for x in grid.grid_scores_]\n",
    "scores = np.array(scores).reshape(len(pen), len(C_range))\n",
    "scores = np.transpose(scores)\n",
    "\n",
    "# Make a heatmap with the performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(scores, interpolation='nearest', origin='higher', cmap=plt.cm.get_cmap('jet_r'))\n",
    "plt.xticks(np.arange(len(pen)), pen)\n",
    "plt.yticks(np.arange(len(C_range)), C_range)\n",
    "plt.xlabel('penalisation norm')\n",
    "plt.ylabel('inv regularisation strength')\n",
    "\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Classification Accuracy', rotation=270, labelpad=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try these out to see how the performance metrics are affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_regression = LogisticRegression(C=grid.best_params_['C'], penalty=grid.best_params_['penalty'])\n",
    "l_regression.fit(XTrain, yTrain)\n",
    "l_prediction = l_regression.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, l_prediction)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, l_prediction),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on cross-validating and tuning logistic regression models, see: <br/>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n",
    "and <br/>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Neural Networks\n",
    "\n",
    "A neural network is a set of connected input-output units. During training, the connections are assigned different weights. This allows the classification function to take on highly complex \"shapes\" (equivalent to complicated mathematical expressions that go beyond the linear or polynomial models of logistic regression). This might also mean that the resulting model is difficult to interpret and map to domain knowledge. (NB. even though you might think of the second layer of a neural network as just a logistic regression model, the non-linear transformation in the hidden units gives the input to output mapping a non-linear decision boundary.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################################################################  \n",
    "# Write your code here \n",
    "# 1. Build the Neural Net classifier classifier using the default parameters\n",
    "# 2. Train (fit) the model\n",
    "# 3. Test (predict)\n",
    "# 4. Report the performance metrics\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# Solution #\n",
    "nnet = multilayer_perceptron.MultilayerPerceptronClassifier(activation='logistic',\n",
    "                                                            hidden_layer_sizes=2, learning_rate_init=.5)\n",
    "nnet.fit(XTrain, yTrain)\n",
    "net_prediction = nnet.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, net_prediction)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, net_prediction),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the classification boundary created by the neural network using the built in visualisation function `nnDecisionPlot`. As with the above examples, only the test samples have been included in the plot. And remember that the decision boundary has been built using the _training_ data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visplots.nnDecisionPlot(XTrain, yTrain, XTest, yTest, 2, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visplots.nnDecisionPlot(XTrain, yTrain, XTest, yTest, (2,3,6), .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Neural Nets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Neural networks have many hyperparameters, all of which could potentially be tuned, including learning rate, loss function, number of training iterations, number of hidden layers and number of units within each of them, nonlinearity function, and weight initialisation. \n",
    "\n",
    "Here's a worked through example which explores the set of parameter configurations with different numbers of hidden layers and units within them (`hidden_layer_sizes`), and learning rates (`learning_rate_init`).\n",
    "\n",
    "Note the syntax to specify the number of hidden layers and units with them. If a tuple is given, each value in the tuple stands for the number of units in a layer, e.g. the tuple `(2,3,4)` would mean a network with two units in the first layer, three units in the second, and four in the third. If a single value is given, then there is only one hidden layer, and the value stands for the number of units in this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the parameters to be optimised and their values/ranges\n",
    "# Range for gamma and Cost hyperparameters\n",
    "layer_size_range = [(3,2),(10,10),(2,2,2),10,5] # different networks shapes\n",
    "learning_rate_range = np.linspace(.1,1,3)\n",
    "\n",
    "parameters = [{'hidden_layer_sizes': layer_size_range, 'learning_rate_init': learning_rate_range}]\n",
    "\n",
    "grid = GridSearchCV(multilayer_perceptron.MultilayerPerceptronClassifier(), parameters, cv= 10)\n",
    "grid.fit(XTrain, yTrain)\n",
    "\n",
    "best_size    = grid.best_params_['hidden_layer_sizes']\n",
    "best_best_lr = grid.best_params_['learning_rate_init']\n",
    "\n",
    "print \"The best parameters are: hidden_layer_sizes=\", best_size, \" and learning_rate_init=\", best_best_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try these out to see how the performance metrics are affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnet = multilayer_perceptron.MultilayerPerceptronClassifier(hidden_layer_sizes=best_size, learning_rate_init=best_best_lr)\n",
    "nnet.fit(XTrain, yTrain)\n",
    "net_prediction = nnet.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, net_prediction)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, net_prediction),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results of the grid search using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grid_scores_ contains parameter settings and scores\n",
    "scores = [x[1] for x in grid.grid_scores_]\n",
    "scores = np.array(scores).reshape(len(layer_size_range), len(learning_rate_range))\n",
    "scores = np.transpose(scores)\n",
    "\n",
    "# Make a heatmap with the performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(scores, interpolation='nearest', origin='higher', cmap=plt.cm.get_cmap('jet_r'))\n",
    "plt.xticks(np.arange(len(layer_size_range)), layer_size_range)\n",
    "plt.yticks(np.arange(len(learning_rate_range)), learning_rate_range)\n",
    "plt.xlabel('hidden layer topology')\n",
    "plt.ylabel('learning rate')\n",
    "\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Classification Accuracy', rotation=270, labelpad=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *So, what is your best technique and why?*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
